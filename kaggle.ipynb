{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-04T20:23:53.713085Z","iopub.status.busy":"2023-01-04T20:23:53.712674Z","iopub.status.idle":"2023-01-04T20:24:05.645033Z","shell.execute_reply":"2023-01-04T20:24:05.643686Z","shell.execute_reply.started":"2023-01-04T20:23:53.713055Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["# !pip install gymnasium\n","print(\"Installed gymnasium\")"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-01-04T20:24:05.648432Z","iopub.status.busy":"2023-01-04T20:24:05.648071Z","iopub.status.idle":"2023-01-04T20:24:05.673256Z","shell.execute_reply":"2023-01-04T20:24:05.672067Z","shell.execute_reply.started":"2023-01-04T20:24:05.648396Z"},"trusted":true},"outputs":[],"source":["import gymnasium as gym\n","from gymnasium import spaces\n","import pandas as pd\n","import numpy as np\n","\n","class MultiDataCenterEnvironment(gym.Env):\n","  metadata = {\"render_modes\": [\"human\"]}\n","\n","  def _generate_workload(self):\n","    cpu_requirement = np.random.rand() * 100\n","    return np.float32(cpu_requirement)\n","  \n","  def _get_workload_datacentre(self, epsilon=0.5):\n","    r = np.random.rand()\n","    data_centre_to_send_work = self._workload_datacentre\n","    if r <= epsilon:\n","      data_centre_to_send_work = 1 - data_centre_to_send_work\n","    return data_centre_to_send_work\n","\n","  def __init__(self, machines_data, datacentre_mapping, num_datacentres):\n","    super(MultiDataCenterEnvironment, self).__init__()\n","    \n","    assert len(machines_data) > 0\n","    assert len(machines_data[0]) > 0\n","    assert num_datacentres <= len(machines_data)\n","\n","    self.num_datacentres = num_datacentres\n","    self.machines_data = machines_data\n","    self.datacentre_mapping = datacentre_mapping\n","\n","    # Current time of the simulation\n","    self.current_time = 0\n","    # Current state of the simulation\n","    self._workload = self._generate_workload()\n","    self._workload_datacentre = 0\n","    self._machines_curr_state: list[np.float32] = [machine_data[0] for machine_data in self.machines_data]\n","\n","    # (cpu1, cpu2, ..., cpu10, workload_requirement, data_center)\n","    self.observation_space = spaces.Dict(\n","      {\n","        \"machines_curr_state\": spaces.Tuple((spaces.Box(low=0, high=100, dtype=np.float32) for _ in range(len(machines_data)))),\n","        \"workload\": spaces.Box(low=0, high=100, dtype=np.float32),\n","        \"workload_datacentre\": spaces.Discrete(num_datacentres),\n","      }\n","    )\n","    \n","    # One action per machine and a no-op; choose to send the workload to a machine, or choose not to do anything\n","    self.action_space = spaces.Discrete(len(machines_data) + 1)\n","\n","  def reset(self, seed=None, options=None):\n","    # We need the following line to seed self.np_random\n","    super().reset(seed=seed)\n","\n","    # Current time of the simulation\n","    self.current_time = 0\n","    # Current state of the simulation\n","    self._workload = self._generate_workload()\n","    self._workload_datacentre = self._get_workload_datacentre()\n","    self._machines_curr_state: list[np.float32] = [np.array([machine_data[0]]) for machine_data in self.machines_data]\n","\n","    observation = {\n","      \"machines_curr_state\": tuple(self._machines_curr_state),\n","      \"workload\": np.array([self._workload]),\n","      \"workload_datacentre\": self._workload_datacentre,\n","    }\n","    info = {} # Not sure what to use this for\n","\n","    return observation, info\n","  \n","  # 10 machines, action A0 - A9, choose, no machine\n","  def step(self, action):\n","    observation = None\n","    reward = 0\n","    done = self.current_time >= (len(self.machines_data[0]) - 2)\n","    info = {} # Not sure what to use this for\n","\n","    # Apply the action\n","    if action == 10:\n","      # Do nothing\n","      reward = 0\n","    else:\n","      machine_picked = action\n","      spare_capacity = 100 - self._machines_curr_state[machine_picked]\n","      # reward = min(100, int(spare_capacity / self._workload) * 100)\n","      if spare_capacity >= self._workload:\n","          reward = 100\n","      else:\n","        reward = int((spare_capacity / self._workload) * 100)\n","\n","      # Half the reward if the datacentre is not where the workload originated\n","      if self._workload_datacentre != self.datacentre_mapping[machine_picked]:\n","        reward /= 2\n","    \n","    # Advance the time\n","    self.current_time += 1\n","    # Advance the state of the simulation\n","    self._workload = self._generate_workload()\n","    self._workload_datacentre = self._get_workload_datacentre()\n","    self._machines_curr_state: list[np.float32] = [np.array([machine_data[self.current_time]]) for machine_data in self.machines_data]\n","\n","    observation = {\n","      \"machines_curr_state\": tuple(self._machines_curr_state),\n","      \"workload\": np.array([self._workload]),\n","      \"workload_datacentre\": self._workload_datacentre,\n","    }\n","\n","    return observation, reward, done, False, info\n","    \n","print(\"Loaded environment\")\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-04T20:24:05.675371Z","iopub.status.busy":"2023-01-04T20:24:05.675016Z","iopub.status.idle":"2023-01-04T20:24:05.701760Z","shell.execute_reply":"2023-01-04T20:24:05.700811Z","shell.execute_reply.started":"2023-01-04T20:24:05.675341Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["import random\n","from collections import deque\n","\n","import torch\n","import torch.nn.functional as F\n","from gymnasium.core import Env\n","from torch import nn\n","\n","class ReplayBuffer():\n","    def __init__(self, size:int, early_transitions_buffer_percentage:int = 0.1):\n","        \"\"\"Replay buffer initialisation\n","\n","        Args:\n","            size: maximum numbers of objects stored by replay buffer\n","        \"\"\"\n","        self.size = size\n","        self.early_transitions_buffer_percentage = early_transitions_buffer_percentage\n","        self.early_transitions_buffer_size = int(early_transitions_buffer_percentage * size)\n","        # Reserve part of the buffer (default 10%) separate; do not overwrite any transitions here once filled\n","        self.early_transitions_buffer = deque([], self.early_transitions_buffer_size)\n","        self.rest_buffer_size = size - self.early_transitions_buffer_size\n","        self.rest_buffer = deque([], self.rest_buffer_size)\n","    \n","    def get_curr_size(self):\n","        return len(self.early_transitions_buffer) + len(self.rest_buffer)\n","\n","    \n","    def push(self, transition):\n","        \"\"\"Push an object to the replay buffer\n","\n","        Args:\n","            transition: object to be stored in replay buffer. Can be of any type\n","        \n","        Returns:\n","            The current memory of the buffer (any iterable object e.g. list)\n","        \"\"\"\n","        # Fill up the early_transitions_buffer until it is full\n","        if len(self.early_transitions_buffer) < self.early_transitions_buffer_size:\n","            self.early_transitions_buffer.append(transition)\n","        # If the early_transitions_buffer is full, we use the rest_buffer as usual\n","        else:\n","            self.rest_buffer.append(transition)\n","        return list(self.early_transitions_buffer) + list(self.rest_buffer)\n","\n","    def sample(self, batch_size:int):\n","        \"\"\"Get a random sample from the replay buffer\n","        \n","        Args:\n","            batch_size: size of sample\n","\n","        Returns:\n","            iterable (e.g. list) with objects sampled from buffer without replacement\n","        \"\"\"\n","        # Return a random sample from both buffers combined\n","        return random.sample(list(self.early_transitions_buffer) + list(self.rest_buffer), batch_size)\n","\n","\n","class DQN(nn.Module):\n","    def __init__(self, layer_sizes):\n","        \"\"\"\n","        DQN initialisation\n","\n","        Args:\n","            layer_sizes: list with size of each layer as elements\n","        \"\"\"\n","        super(DQN, self).__init__()\n","        # torch.manual_seed(14597905165985114927) - This is a bad seed for a network of [4, 256, 2]\n","        self.layers = nn.ModuleList([nn.Linear(layer_sizes[i], layer_sizes[i+1]) for i in range(len(layer_sizes)-1)])\n","    \n","    def forward (self, x:torch.Tensor)->torch.Tensor:\n","        \"\"\"Forward pass through the DQN\n","\n","        Args:\n","            x: input to the DQN\n","        \n","        Returns:\n","            outputted value by the DQN\n","        \"\"\"\n","        for layer in self.layers:\n","            x = F.leaky_relu(layer(x))\n","        return x\n","\n","def greedy_action(dqn:DQN, state:torch.Tensor)->int:\n","    \"\"\"Select action according to a given DQN\n","    \n","    Args:\n","        dqn: the DQN that selects the action\n","        state: state at which the action is chosen\n","\n","    Returns:\n","        Greedy action according to DQN\n","    \"\"\"\n","    return int(torch.argmax(dqn(state)))\n","\n","def epsilon_greedy(epsilon:float, dqn:DQN, state:torch.Tensor)->int:\n","    \"\"\"Sample an epsilon-greedy action according to a given DQN\n","    \n","    Args:\n","        epsilon: parameter for epsilon-greedy action selection\n","        dqn: the DQN that selects the action\n","        state: state at which the action is chosen\n","    \n","    Returns:\n","        Sampled epsilon-greedy action\n","    \"\"\"\n","    q_values = dqn(state)\n","    num_actions = q_values.shape[0]\n","    greedy_act = int(torch.argmax(q_values))\n","    p = float(torch.rand(1))\n","    if p>epsilon:\n","        return greedy_act\n","    else:\n","        return random.randint(0,num_actions-1)\n","\n","def update_target(target_dqn:DQN, policy_dqn:DQN):\n","    \"\"\"Update target network parameters using policy network.\n","    Does not return anything but modifies the target network passed as parameter\n","    \n","    Args:\n","        target_dqn: target network to be modified in-place\n","        policy_dqn: the DQN that selects the action\n","    \"\"\"\n","\n","    target_dqn.load_state_dict(policy_dqn.state_dict())\n","\n","def loss(policy_dqn:DQN, target_dqn:DQN,\n","         states:torch.Tensor, actions:torch.Tensor,\n","         rewards:torch.Tensor, next_states:torch.Tensor, dones:torch.Tensor, ddqn = False)->torch.Tensor:\n","    \"\"\"Calculate Bellman error loss\n","    \n","    Args:\n","        policy_dqn: policy DQN\n","        target_dqn: target DQN\n","        states: batched state tensor\n","        actions: batched action tensor\n","        rewards: batched rewards tensor\n","        next_states: batched next states tensor\n","        dones: batched Boolean tensor, True when episode terminates\n","    \n","    Returns:\n","        Float scalar tensor with loss value\n","    \"\"\"\n","    if ddqn:\n","        # DDQN Start:\n","        # torch.Size([BATCH_SIZE, 1])\n","        policy_dqn_best_action_batch = torch.argmax(policy_dqn(next_states), dim=1).reshape(-1, 1) # select best actions based on policy dqn\n","        # torch.Size([BATCH_SIZE, 2])\n","        target_dqn_next_states = target_dqn(next_states)\n","        # torch.Size([BATCH_SIZE, 1])\n","        selected_action_batch = target_dqn_next_states.gather(dim=1, index=policy_dqn_best_action_batch) # use actions on the target dqn\n","\n","        # dones shape: torch.Size([BATCH_SIZE, 1]), reshaped(-1): torch.Size.([BATCH_SIZE])\n","        # rewards shape: torch.Size([BATCH_SIZE, 1]), reshaped(-1): torch.Size([BATCH_SIZE])\n","        # selected_action_batch reshaped(-1): torch.Size([BATCH_SIZE])\n","        # bellman_targets shape: torch.Size([BATCH_SIZE])\n","        bellman_targets = (~dones).reshape(-1)*(selected_action_batch).reshape(-1) + rewards.reshape(-1)\n","        # DDQN End\n","    else:\n","        bellman_targets = (~dones).reshape(-1)*(target_dqn(next_states)).max(1).values + rewards.reshape(-1)\n","\n","    q_values = policy_dqn(states).gather(1, actions).reshape(-1)\n","    return ((q_values - bellman_targets)**2).mean()\n","\n","print(\"Loaded utils\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-04T20:24:05.705381Z","iopub.status.busy":"2023-01-04T20:24:05.704944Z","iopub.status.idle":"2023-01-04T20:24:05.779640Z","shell.execute_reply":"2023-01-04T20:24:05.778123Z","shell.execute_reply.started":"2023-01-04T20:24:05.705335Z"},"trusted":true},"outputs":[],"source":["import os\n","import pandas as pd\n","import numpy as np\n","\n","# Set up environment\n","\n","machines_data = []\n","\n","print (\"Opening files machine data files:\")\n","path_to_machine_data = os.path.join(os.getcwd(), \"..\", \"input\", \"machine-data\")\n","for filename in os.listdir(path_to_machine_data):\n","  df = pd.read_csv(os.path.join(path_to_machine_data, filename))\n","  machine_name = os.path.splitext(filename)[0]\n","#   print(machine_name)\n","#   print(df.head())\n","  machine_data = [np.int32(row[1]) for row in df.values.tolist()]\n","  machines_data.append(machine_data)\n","\n","num_datacentres = 2\n","datacentre_mapping = {\n","  0: 0, 1: 0, 2: 0, 3: 0, 4: 0,\n","  5: 1, 6: 1, 7: 1, 8: 1, 9: 1,\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-04T20:24:05.782309Z","iopub.status.busy":"2023-01-04T20:24:05.781470Z","iopub.status.idle":"2023-01-04T20:24:05.791575Z","shell.execute_reply":"2023-01-04T20:24:05.790032Z","shell.execute_reply.started":"2023-01-04T20:24:05.782266Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import math\n","import numpy as np\n","\n","import gymnasium as gym\n","import matplotlib.pyplot as plt\n","print(\"Imports\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-04T20:24:50.668656Z","iopub.status.busy":"2023-01-04T20:24:50.667349Z","iopub.status.idle":"2023-01-04T20:24:51.275359Z","shell.execute_reply":"2023-01-04T20:24:51.274275Z","shell.execute_reply.started":"2023-01-04T20:24:50.668614Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["# Test main code\n","# env = gym.make('custom_environments/MultiDataCenterEnvironment', \n","#                     machines_data=machines_data, \n","#                     datacentre_mapping=datacentre_mapping, \n","#                     num_datacentres=num_datacentres\n","#                   )\n","env = MultiDataCenterEnvironment(\n","                    machines_data=machines_data, \n","                    datacentre_mapping=datacentre_mapping, \n","                    num_datacentres=num_datacentres\n","                  )\n","\n","\n","terminated = False\n","done = False\n","env.reset()\n","time = 0\n","random_action_ret = 0\n","while not (done or terminated):\n","  random_action = np.random.randint(0, 11)\n","  observation, reward, done, terminated, info = env.step(random_action)\n","  flattened_observation = np.concatenate((\n","                                            (np.array(observation[\"machines_curr_state\"]) / 100).ravel(), \n","                                            (observation[\"workload\"] / 100).ravel(), \n","                                            np.array([0. if (observation[\"workload_datacentre\"] == 0) else 1.])\n","                                        ))\n","#   flattened_normalised_observation = (flattened_observation-np.min(flattened_observation))/(np.max(flattened_observation)-np.min(flattened_observation))\n","  print(time)\n","  print(flattened_observation)\n","  print(\"Reward: \", reward)\n","  random_action_ret += reward\n","  time += 1\n","\n","print (\"Random Action Return:\", random_action_ret)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-04T20:25:04.785580Z","iopub.status.busy":"2023-01-04T20:25:04.784878Z","iopub.status.idle":"2023-01-04T20:25:04.922962Z","shell.execute_reply":"2023-01-04T20:25:04.921734Z","shell.execute_reply.started":"2023-01-04T20:25:04.785513Z"},"trusted":true},"outputs":[],"source":["# Test main code\n","# env = gym.make('custom_environments/MultiDataCenterEnvironment', \n","#                     machines_data=machines_data, \n","#                     datacentre_mapping=datacentre_mapping, \n","#                     num_datacentres=num_datacentres\n","#                   )\n","env = MultiDataCenterEnvironment(\n","                    machines_data=machines_data, \n","                    datacentre_mapping=datacentre_mapping, \n","                    num_datacentres=num_datacentres\n","                  )\n","\n","\n","terminated = False\n","done = False\n","env.reset()\n","time = 0\n","greedy_action_ret = 0\n","while not (done or terminated):\n","  greedy_action = 10\n","  most_spare_capacity = 0\n","  for i, machine_data in enumerate(machines_data):\n","    spare_capacity = 100 - machine_data[time]\n","    if spare_capacity > most_spare_capacity:\n","        most_spare_capacity = spare_capacity\n","        greedy_action = i\n","    \n","\n","  observation, reward, done, terminated, info = env.step(greedy_action)\n","  flattened_observation = np.concatenate((\n","                                            (np.array(observation[\"machines_curr_state\"]) / 100).ravel(), \n","                                            (observation[\"workload\"] / 100).ravel(), \n","                                            np.array([0. if (observation[\"workload_datacentre\"] == 0) else 1.])\n","                                        ))\n","#   flattened_normalised_observation = (flattened_observation-np.min(flattened_observation))/(np.max(flattened_observation)-np.min(flattened_observation))\n","#   print(time)\n","#   print(flattened_observation)\n","#   print(\"Reward: \", reward)\n","  greedy_action_ret += reward\n","  time += 1\n","\n","print (\"Greedy Action Return:\", greedy_action_ret)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-03T13:46:02.998863Z","iopub.status.busy":"2023-01-03T13:46:02.998483Z","iopub.status.idle":"2023-01-03T13:46:03.023249Z","shell.execute_reply":"2023-01-03T13:46:03.021380Z","shell.execute_reply.started":"2023-01-03T13:46:02.998833Z"},"trusted":true},"outputs":[],"source":["def run_experiment(\n","        NUM_RUNS = 1, \n","        EPSILON_DECAY=0.97, \n","        HIDDEN_LAYER_SIZE=512, \n","        LEARNING_RATE = 0.01, \n","        REPLAY_BUFFER_SIZE=30_000,\n","        NUM_EPISODES=300,\n","        REPLAY_BUFFER_MINIBATCH_SIZE=64,\n","        NETWORK_SYNC_FREQUENCY=50,\n","        ddqn=False\n","    ):\n","    print(\"DDQN:\", ddqn)\n","    EPSILON_START = 1\n","    EPSILON_MIN = 0.001\n","\n","    # Hyperparameters\n","    INPUT_LAYER_SIZE = 12 # 10 machines + workload + workload datacentre\n","    OUTPUT_LAYER_SIZE = 11 # 10 machines (1 action per machine) + noop action\n","    POLICY_NET_LAYER_SIZES = [INPUT_LAYER_SIZE, HIDDEN_LAYER_SIZE, OUTPUT_LAYER_SIZE]\n","    TARGET_NET_LAYER_SIZES = [INPUT_LAYER_SIZE, HIDDEN_LAYER_SIZE, OUTPUT_LAYER_SIZE]\n","    LEARNING_RATE = LEARNING_RATE\n","    REPLAY_BUFFER_SIZE = REPLAY_BUFFER_SIZE\n","    REPLAY_BUFFER_MINIBATCH_SIZE = REPLAY_BUFFER_MINIBATCH_SIZE\n","    NETWORK_SYNC_FREQUENCY = NETWORK_SYNC_FREQUENCY\n","    \n","    env = MultiDataCenterEnvironment(\n","                    machines_data=machines_data, \n","                    datacentre_mapping=datacentre_mapping, \n","                    num_datacentres=num_datacentres\n","                  )\n","    runs_results = []\n","    \n","    for run in range(NUM_RUNS):\n","        print(f\"Starting run {run+1} of {NUM_RUNS}\")\n","        policy_net = DQN(POLICY_NET_LAYER_SIZES)\n","        target_net = DQN(TARGET_NET_LAYER_SIZES)\n","        EPSILON = EPSILON_START\n","        update_target(target_net, policy_net)\n","        target_net.eval()\n","\n","        optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n","        memory = ReplayBuffer(REPLAY_BUFFER_SIZE)\n","\n","        steps_done = 0\n","\n","        episode_returns = []\n","\n","        for i_episode in range(NUM_EPISODES):\n","            print(\"episode \", i_episode+1, \"/\", NUM_EPISODES)\n","\n","            observation, info = env.reset()\n","            flattened_observation = np.concatenate((\n","                                            (np.array(observation[\"machines_curr_state\"]) / 100).ravel(), \n","                                            (observation[\"workload\"] / 100).ravel(), \n","                                            np.array([0. if (observation[\"workload_datacentre\"] == 0) else 1.]) # one hot encode the datacentres\n","                                        ))\n","            state = torch.tensor(flattened_observation).float()\n","\n","            done = False\n","            terminated = False\n","            ret = 0\n","            while not (done or terminated):\n","\n","                # Select and perform an action\n","                action = epsilon_greedy(EPSILON, policy_net, state)\n","\n","                observation, reward, done, terminated, info = env.step(action)\n","                ret += reward\n","                flattened_observation = np.concatenate((\n","                                            (np.array(observation[\"machines_curr_state\"]) / 100).ravel(), \n","                                            (observation[\"workload\"] / 100).ravel(), \n","                                            np.array([0. if (observation[\"workload_datacentre\"] == 0) else 1.]) # one hot encode the datacentres\n","                                        ))\n","                reward = torch.tensor([reward])\n","                action = torch.tensor([action])\n","                next_state = torch.tensor(flattened_observation).reshape(-1).float()\n","\n","                memory.push([state, action, next_state, reward, torch.tensor([done])])\n","\n","                # Move to the next state\n","                state = next_state\n","\n","                steps_done += 1\n","\n","                # Perform one step of the optimization (on the policy network)\n","                if not memory.get_curr_size() < REPLAY_BUFFER_MINIBATCH_SIZE:\n","                    transitions = memory.sample(REPLAY_BUFFER_MINIBATCH_SIZE)\n","                    state_batch, action_batch, nextstate_batch, reward_batch, dones = (torch.stack(x) for x in zip(*transitions))\n","                    # Compute loss\n","                    mse_loss = loss(policy_net, target_net, state_batch, action_batch, reward_batch, nextstate_batch, dones, ddqn=ddqn)\n","                    # Optimize the model\n","                    optimizer.zero_grad()\n","                    mse_loss.backward()\n","                    optimizer.step()\n","\n","                if steps_done % NETWORK_SYNC_FREQUENCY == 0:\n","                    update_target(target_net, policy_net)\n","\n","                if done or terminated:\n","                    episode_returns.append(ret)\n","\n","            # Update the target network, copying all weights and biases in DQN\n","            # if i_episode % NETWORK_SYNC_FREQUENCY == 0: \n","            #     update_target(target_net, policy_net)\n","\n","            # Update epsilon\n","            EPSILON = max(EPSILON * EPSILON_DECAY, EPSILON_MIN)\n","        runs_results.append(episode_returns)\n","    print('Complete')\n","    return policy_net, runs_results\n","\n","\n","# def run_trained_policy(policy_net, NUM_RUNS = 5, NUM_EPISODES=300):\n","#     env = gym.make('custom_environments/MultiDataCenterEnvironment', \n","#                     machines_data=machines_data, \n","#                     datacentre_mapping=datacentre_mapping, \n","#                     num_datacentres=num_datacentres\n","#                   )\n","#     runs_results = []\n","\n","#     for run in range(NUM_RUNS):\n","#         print(f\"Starting run {run+1} of {NUM_RUNS}\")\n","#         steps_done = 0\n","\n","#         episode_durations = []\n","\n","#         for i_episode in range(NUM_EPISODES):\n","#             if (i_episode+1) % 50 == 0:\n","#                 print(\"episode \", i_episode+1, \"/\", NUM_EPISODES)\n","\n","#             observation, info = env.reset()\n","#             state = torch.tensor(observation).float()\n","\n","#             done = False\n","#             terminated = False\n","#             t = 0\n","#             while not (done or terminated):\n","#                 # Select and perform an action\n","#                 action = epsilon_greedy(0, policy_net, state)\n","\n","#                 observation, reward, done, terminated, info = env.step(action)\n","#                 next_state = torch.tensor(observation).reshape(-1).float()\n","\n","#                 # Move to the next state\n","#                 state = next_state\n","\n","#                 steps_done += 1\n","\n","#                 if done or terminated:\n","#                     episode_durations.append(t + 1)\n","#                 t += 1\n","\n","#         runs_results.append(episode_durations)\n","#     print('Complete')\n","#     return runs_results\n","\n","print (\"Defined run_experiment\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-03T13:54:29.912611Z","iopub.status.busy":"2023-01-03T13:54:29.911378Z","iopub.status.idle":"2023-01-03T13:54:29.926843Z","shell.execute_reply":"2023-01-03T13:54:29.924633Z","shell.execute_reply.started":"2023-01-03T13:54:29.912556Z"},"trusted":true},"outputs":[],"source":["def plot_graph(ddqn_runs_results, NUM_EPISODES, filename):\n","    ddqn_results = torch.tensor(ddqn_runs_results)\n","    ddqn_means = ddqn_results.float().mean(0)\n","    ddqn_stds = ddqn_results.float().std(0)\n","\n","    plt.plot(torch.arange(NUM_EPISODES), ddqn_means, label=\"DDQN Mean\", color='orange')\n","    plt.ylabel(\"return\")\n","    plt.xlabel(\"episode\")\n","\n","    plt.fill_between(np.arange(NUM_EPISODES), ddqn_means + ddqn_stds, ddqn_means - ddqn_stds, alpha=0.3, label=f\"DDQN Mean ± Std\", color='orange')\n","    plt.axhline(y=random_action_ret, color='r', linestyle='--')\n","    plt.legend(loc='lower right')\n","    plt.savefig(f'/kaggle/working/{filename}.png')\n","    plt.show()\n","\n","print(\"Defined plot_graph\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-03T13:54:31.910475Z","iopub.status.busy":"2023-01-03T13:54:31.910037Z","iopub.status.idle":"2023-01-03T14:00:13.455900Z","shell.execute_reply":"2023-01-03T14:00:13.454663Z","shell.execute_reply.started":"2023-01-03T13:54:31.910440Z"},"trusted":true},"outputs":[],"source":["# Hyperparameter search\n","# hidden_layer_sizes = [128, 256, 512]\n","# learning_rates = [0.0001, 0.0005, 0.001, 0.005, 0.01]\n","# epsilon_decays = [0.999, 0.995, 0.99, 0.98, 0.97, 0.96]\n","# minibatch_sizes = [32, 64, 128, 256]\n","# network_sync_freqs = [50, 100, 200, 400]\n","\n","hidden_layer_sizes = [512]\n","learning_rates = [0.002]\n","epsilon_decays = [0.98]\n","minibatch_sizes = [256]\n","network_sync_freqs = [5000, 7500, 10_000, 15_000, 20_000]\n","\n","num_episodes = 300\n","\n","for hidden_layer_size in hidden_layer_sizes:\n","    for learning_rate in learning_rates:\n","        for epsilon_decay in epsilon_decays:\n","            for minibatch_size in minibatch_sizes:\n","                for network_sync_freq in network_sync_freqs:\n","                    print (\"hidden layer size:\", hidden_layer_size, \"LR:\", learning_rate, \"eps decay:\", epsilon_decay, \"minib size:\", minibatch_size, \"sync freq:\", network_sync_freq)\n","                    ddqn_policy_net, ddqn_runs_results = run_experiment(\n","                                                                NUM_RUNS = 1,\n","                                                                EPSILON_DECAY=epsilon_decay, \n","                                                                HIDDEN_LAYER_SIZE=hidden_layer_size,\n","                                                                LEARNING_RATE = learning_rate,\n","                                                                REPLAY_BUFFER_SIZE=30_000,\n","                                                                NUM_EPISODES=num_episodes,\n","                                                                REPLAY_BUFFER_MINIBATCH_SIZE=minibatch_size,\n","                                                                NETWORK_SYNC_FREQUENCY=network_sync_freq,\n","                                                                ddqn=True\n","                                                            )\n","                    filename = f\"HLS_{hidden_layer_size}_lr_{learning_rate}_epsdec_{epsilon_decay}_MBS_{minibatch_size}_syncfreq_{network_sync_freq}\"\n","                    plot_graph(ddqn_runs_results, num_episodes, filename)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"vscode":{"interpreter":{"hash":"b1613945e647844addf2545e818613ee1909d61cc62972b9823feea8a4433a37"}}},"nbformat":4,"nbformat_minor":4}
